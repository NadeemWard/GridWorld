import numpy as np

def iterative_bellman_equation(grid, actions, policy, discount=1.0, epsilon=10 ** -8):
    '''
        Here, we assume we are using the DP value estimation algorithm as an approximation method for V(s) given policy pi.
      :param policy: The policy to follow pi
      :param reward: the reward function defined by this environment
      :param discount: the discount factor used
      :param epsilon: the stopping criteria, when subsequent iterations lead to a difference less than epsilon we stop
      :return: the approximated state-value estimate V(s)
    '''

    terminal_state = grid.terminal_state # refers to the last state (bottom,right) of a square gridworld
    state_values = np.zeros(terminal_state)  # the V(s) array initialized to zero

    while (True):
        delta = 0 # the way we keep track of successive iterations

        for s in grid.states:

            if (s == terminal_state): continue  # the terminal state so we don't compute it's state-value

            v = state_values[s - 1]

            sum = 0

            for a in actions:

                policy_proba = policy[s][actions.index(a)]

                for s_prime in grid.states:

                    transition_prob = grid.transition_probs.get(s,a,s_prime) # probability of ending up in s_prime given that you are in state s and take action a
                    r = grid.reward_env.get_reward(s, s_prime, a)  # get the reward Rss'a
                    sum += policy_proba * ( transition_prob * (r + discount * state_values[s_prime - 1]) )

            state_values[s - 1] = sum

            delta = max(delta,
                        abs(v - state_values[s - 1]))  # just keeping track of the biggest difference between all states

        if (delta < epsilon): return state_values

def get_return(episode, discount= 1.0):
    '''
    Function to return the complete return for Monte Carlo backup
    :param episode: the episode to get the return from
    :param discount: discount factor
    :return: the Monte Carlo return
    '''
    sum = 0
    index = 0

    for step in episode:
        state, action, reward, next_state = step
        sum += (discount ** index) * reward
        index += 1

    return sum

def monte_carlo(initial_estimates, episodes, discount = 1.0):
    '''
    Monte Carlo state value estimation
    :param initial_estimates: the initial state value estimates
    :param episodes: the episodes generated from an agent (use agent.sample_episodes)
    :param discount: discount factor
    :return: return a list of the state value estimates after every episode
    '''

    estimate_Vs = np.copy(initial_estimates)
    returns = [[] for _ in range(len(initial_estimates))]

    all_estimates = [] # keep track of estimates after every episode
    all_estimates.append(np.copy(estimate_Vs))

    for episode in episodes:

        visited_states = []
        index = 0

        for step in episode:

            state, action, reward, next_state = step

            if (state not in visited_states):

                visited_states.append(state)
                r = get_return(episode[index:], discount=discount)
                returns[state - 1].append(r)
                estimate_Vs[state - 1] = sum(returns[state - 1]) / len(returns[state - 1])

            index+=1

        all_estimates.append(np.copy(estimate_Vs))

    return all_estimates

def n_step_return(n, episode, state_value_estimate, discount=1.0):
    '''
    Function to get n step return in the n-step TD backup
    :param n: number of steps
    :param episode: number of episodes
    :param state_value_estimate: the current state value estimates
    :param discount: the discount factor used
    :return: the n step return
    '''

    td_steps = n
    end_of_episode = False  # variable to indicate whether looking ahead will lead to the end of the episode
    sum = 0

    len_episode = len(episode)
    if len_episode <= n:  # length of episode is less than n.
        td_steps = len_episode  # just stop at the end of the episode
        end_of_episode = True

    for index in range(td_steps):
        state, action, reward, next_state = episode[index]
        sum += (discount ** index) * reward

    if not end_of_episode:
        state, action, reward, next_state = episode[td_steps]
        sum += (discount ** (td_steps)) * state_value_estimate[state - 1]

    return sum

def n_step_td(initial_estimates, n, episodes, discount=1.0, learning_rate=0.2):
    '''
    N step TD state value estimation
    :param initial_estimates: initial state value estimates
    :param n: the number of steps in the TD method
    :param episodes: the epsisodes generated by the agent
    :param discount: discount factor
    :param learning_rate: the learning rate used for the update
    :return: return a list of state value estimates after each episode
    '''
    state_value_estimate = np.copy(initial_estimates)  # the V(s) array initialized to zero
    all_estimates = []
    all_estimates.append(np.copy(state_value_estimate))

    # loop through episodes
    for episode in episodes:

        index = 0  # which episode we are at

        for step in episode:
            state, action, reward, next_state = step

            state_value_estimate[state - 1] = state_value_estimate[state - 1] + learning_rate * (
                n_step_return(n, episode[index:], state_value_estimate, discount=discount) - state_value_estimate[
                    state - 1])

            index += 1

        all_estimates.append(np.copy(state_value_estimate))

    return all_estimates

def td_lambda(initial_estimates,  _lambda, episodes , discount=1.0, learning_rate=0.2):
    '''
    TD(lambda) state value estimation
    :param initial_estimates: initial state values
    :param _lambda: the lambda used for this TD(.)
    :param episodes: episodes generated by the agent
    :param discount: discount factor
    :param learning_rate: learning rate
    :return: a list of state value estimates for each episode
    '''

    gridworld_size = len(initial_estimates)
    state_value_estimate = np.copy(initial_estimates)
    e = np.zeros(len(state_value_estimate))  # the eligibility trace
    all_estimates = []
    all_estimates.append(np.copy(state_value_estimate))


    for episode in episodes:

        for step in episode:

            state, action, reward, next_state = step

            delta = reward + discount * state_value_estimate[next_state - 1] - state_value_estimate[state - 1]
            e[state - 1] = e[state - 1] + 1

            for s in range(gridworld_size):
                state_value_estimate[s] = state_value_estimate[s] + learning_rate * delta * e[s]
                e[s] = discount * _lambda * e[s]

        all_estimates.append(np.copy(state_value_estimate))


    return all_estimates

def avg_abs_diff(state_values, estimates):

    average_absolute_difference = []

    for episode_estimates in estimates:
        average_absolute_difference.append(np.average(abs(state_values - episode_estimates)))

    return average_absolute_difference

if __name__ == "__main__":


    from Grid import GridWorld
    from Transitions import Transitions_Probs
    from Rewards import Reward
    from Agent import Agent
    #import numpy as np

    # Define the gridworld
    height = 5
    width = 5
    terminal_states = [1, 5]

    grid = GridWorld(height, width)
    grid.print_grid()  # print function to show your grid

    # Define the environment dynamics
    actions = ["up", "down", "right", "left"]

    tp = Transitions_Probs(grid, actions)
    tp.create_common_transition("Deterministic")  # There are 3 choices for common transition probabilities ("Bernoulli",0.7)) # "Deterministic"
    tp.add_terminal_states(terminal_states)

    defined_reward = {1: 1, 5: 10}  # Here, at state 1 I have a reward of 1 and at state 4 I have a reward of 10.
    reward_env = Reward(grid, actions)
    reward_env.common_reward(defined_reward)
    discount = 0.9

    policy = np.ones((len(grid.states), len(actions))) * 0.25  # uniform policy
    agent = Agent(grid, actions, policy)
    #agent.start_state = 25

    episodes = agent.sample_trajectory(start_state=25,horizon=100 ) #, terminal_states=terminal_states)  # generate the episodes
    print(episodes)














    exit(0)

    height = 4  # square gridworld
    width = 4

    import Grid

    grid = Grid.GridWorld(height, width)
    grid.print_grid()

    actions = ["up", "down", "right", "left"]

    import Transitions
    x = Transitions.Transitions_Probs(grid, actions)
    x.create_common_transition("Deterministic")  # ("Bernoulli",0.7)) # "Deterministic"

    import Rewards
    sparse_reward = Rewards.Reward(grid, actions)
    sparse_reward.common_reward("sparse")

    discount = 0.2
    policy = np.ones((len(grid.states), len(actions) )) * 0.25 # uniform policy

    state_values = iterative_bellman_equation(grid, actions, policy, discount=discount) # approximate the true state values

    from Agent import Agent as agent
    episodes = agent(grid,actions,policy).sample_episode(1000,terminal_state = 16) # generate the episodes

    initial_states = np.zeros(len(grid.states)) # initial estimates should be an numpy array

    mc_estimates = monte_carlo(initial_states, episodes, discount=discount)

    n_estimates = n_step_td(initial_states,50,episodes, discount= discount,learning_rate=0.001)

    lambda_estimate = td_lambda(initial_states,0,episodes,discount=discount,learning_rate=0.001)

    n_aad = avg_abs_diff(state_values,n_estimates)
    mc_aad = avg_abs_diff(state_values, mc_estimates)
    lambda_aad = avg_abs_diff(state_values,lambda_estimate)

    import matplotlib.pyplot as plt
    plt.plot(mc_aad)
    plt.plot(n_aad)
    plt.plot(lambda_aad)
    plt.ylabel("Average absolute difference")
    plt.xlabel("# of episodes")
    plt.show()


    exit(0)
